#!/bin/bash -ex
set -o pipefail

#
# Run RCCL tests
#

rm -rf run.sh 
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun
\$WITH_CONDA
set -x

# -b minbytes
# -e maxbytes
# -f increment factor
# -g gpus per thread
/opt/rccltests/all_reduce_perf -z 1 -b 2M -e 2048M -f 2 -g 1 -t 1 -R 1 -n 80 -w 5 -d half

# python -c "print('hello')"
EOF
chmod +x run.sh 

$SCMD \
    -B $(pwd):/myrun \
    $1 \
    /myrun/run.sh |& tee res.log

# Check BW for large transfers which are more stable
for i in $(grep -r 'half     sum' res.log | tail -n5 | awk '{print $7}') ; do
  echo "Measure RCCL test all-reduce BW to be $i..."
  if (( $(echo "$i < 29.00" |bc -l) )); then
    echo "Too low!!!";
    exit 1
  fi
done

#
# Run MNIST test
#
cat > mnist.py << EOF
import os
from datetime import datetime
import argparse
import torch.multiprocessing as mp
import torchvision
import torchvision.transforms as transforms
import torch
import torch.nn as nn
import torch.distributed as dist
from apex.parallel import DistributedDataParallel as DDP
from apex import amp


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-lrank', '--local_rank', default=0, type=int,
                        help='ranking within the nodes')
    parser.add_argument('--epochs', default=2, type=int, metavar='N',
                        help='number of total epochs to run')
                        
    args = parser.parse_args()
    args.world_size = int(os.environ['WORLD_SIZE'])
    args.rank = int(os.environ['RANK'])
    train(args)


class ConvNet(nn.Module):
    def __init__(self, num_classes=10):
        super(ConvNet, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.fc = nn.Linear(7*7*32, num_classes)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        return out


def train( args):
    rank = args.rank
    gpu = args.local_rank
    dist.init_process_group(backend='nccl', init_method='env://', world_size=args.world_size, rank=rank)
    torch.manual_seed(0)
    model = ConvNet()
    torch.cuda.set_device(gpu)
    model.cuda(gpu)
    batch_size = 8
    # define loss function (criterion) and optimizer
    criterion = nn.CrossEntropyLoss().cuda(gpu)
    optimizer = torch.optim.SGD(model.parameters(), 1e-4)
    # Wrap the model
    model = nn.parallel.DistributedDataParallel(model, device_ids=[gpu])
    # Data loading code
    train_dataset = torchvision.datasets.MNIST(root='/datasets',
                                               train=True,
                                               transform=transforms.ToTensor(),
                                               download=False)
    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,
                                                                    num_replicas=args.world_size,
                                                                    rank=rank)
    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                               batch_size=batch_size,
                                               shuffle=False,
                                               num_workers=0,
                                               pin_memory=True,
                                               sampler=train_sampler)

    start = datetime.now()
    total_step = len(train_loader)
    for epoch in range(args.epochs):
        for i, (images, labels) in enumerate(train_loader):
            images = images.cuda(non_blocking=True)
            labels = labels.cuda(non_blocking=True)
            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if (i + 1) % 10 == 0 and rank == 0:
                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, args.epochs, i + 1, total_step,
                                                                         loss.item()))
    if rank == 0:
        print("Training complete in: " + str(datetime.now() - start))


if __name__ == '__main__':
    main()
EOF
cat > mnist-download.py << EOF
import torchvision
import torchvision.transforms as transforms
torchvision.datasets.MNIST(root='/datasets',
                           train=True,
                           transform=transforms.ToTensor(),
                           download=True)
EOF
cat > get-master.py << EOF
import argparse
def get_parser():
    parser = argparse.ArgumentParser(description="Extract master node name from Slurm node list",
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("nodelist", help="Slurm nodelist")
    return parser


if __name__ == '__main__':
    parser = get_parser()
    args = parser.parse_args()

    first_nodelist = args.nodelist.split(',')[0]

    if '[' in first_nodelist:
        a = first_nodelist.split('[')
        first_node = a[0] + a[1].split('-')[0]

    else:
        first_node = first_nodelist

    print(first_node)
EOF

rm -rf run.sh 
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun
\$WITH_CONDA
set -x

  # Download on single rank
  # if [ \$SLURM_PROCID -eq 0 ] ; then
  #   python mnist-download.py
  # fi
  # exit 0

  export NCCL_SOCKET_IFNAME=hsn0,hsn1,hsn2,hsn3
  export MIOPEN_USER_DB_PATH="/tmp/$(whoami)-miopen-cache-\$SLURM_NODEID"
  export MIOPEN_CUSTOM_CACHE_DIR=\$MIOPEN_USER_DB_PATH

  # Set MIOpen cache out of the home folder.
  if [ \$SLURM_LOCALID -eq 0 ] ; then
    rm -rf \$MIOPEN_USER_DB_PATH
    mkdir -p \$MIOPEN_USER_DB_PATH
  fi
  sleep 3
  
  # Report affinity
  echo "Rank \$SLURM_PROCID --> \$(taskset -p \$\$)"
  
  # Set master the first node of the allocation. Also select some port to use and leverage
  # SLURM environment to specify ranks to pytorch DDP.
  export MASTER_ADDR=\$(python get-master.py "\$SLURM_NODELIST")
  export MASTER_PORT=29500
  export WORLD_SIZE=\$SLURM_NPROCS
  export RANK=\$SLURM_PROCID
  
  echo "--> MASTER_ADDR: \$MASTER_ADDR"
  echo "--> MASTER_PORT: \$MASTER_PORT"
  echo "--> WORLD_SIZE: \$WORLD_SIZE"
  echo "--> RANK: \$RANK" 
  
  python -u mnist.py \
    --local_rank \$SLURM_LOCALID \
    --epochs 10
EOF
chmod +x run.sh 

$SCMD \
    -B $(pwd):/myrun \
    -B /pfs/lustrep2/scratch/project_462000125/samantao/data-sets/mnist:/datasets \
    $1 \
    /myrun/run.sh |& tee res.log

#
# Install conda environment
# 
ARG PYTHON_VERSION
RUN $WITH_CONDA; set -eux ; \
  conda create -n pytorch python=$PYTHON_VERSION ; \
  conda activate pytorch ; \
  conda install -y ninja pillow cmake pyyaml
ENV WITH_CONDA "source /opt/miniconda3/bin/activate pytorch"

#
# Install pytorch
# 

# Repository for the wheel files
RUN set -eux ; \
  mkdir /opt/wheels
  
ENV PYTORCH_ROCM_ARCH gfx90a 
ARG PYTORCH_VERSION
ARG PYTORCH_DEBUG
ARG PYTORCH_RELWITHDEBINFO

RUN $WITH_CONDA; set -eux ; \
  if [[ "$PYTORCH_VERSION" != "2.2.2" ]] ; then \
    echo "Assuming Pytorch version 2.2.2" ; \
    false ; \
  fi ; \
  cd /opt/wheels ; \
  ROCM_VERSION_MAJOR=$(echo $ROCM_RELEASE | cut -d'.' -f1) ; \
  ROCM_VERSION_MINOR=$(echo $ROCM_RELEASE | cut -d'.' -f2) ; \
  PYTHON_VERSION_MAJOR=$(echo $PYTHON_VERSION | cut -d'.' -f1) ; \
  PYTHON_VERSION_MINOR=$(echo $PYTHON_VERSION | cut -d'.' -f2) ; \
  pip3 install --pre torch==${PYTORCH_VERSION}+rocm${ROCM_VERSION_MAJOR}.${ROCM_VERSION_MINOR} --index-url https://download.pytorch.org/whl/

#
# Pytorch dependencies
#

RUN $WITH_CONDA; set -eux ; \
  rm -rf $CONDA_PREFIX/lib/libstdc++.so* ; \
  \  
  git clone --recursive https://github.com/ROCmSoftwarePlatform/apex /opt/mybuild ; \
  cd /opt/mybuild ; \
  git checkout -b mydev origin/release/1.1.0 ; \
  git submodule sync ; \
  git submodule update --init --recursive --jobs 0 ; \
  CC=gcc-10, CXX=g++-10 nice python setup.py bdist_wheel --cpp_ext --cuda_ext ; \
  cp -rf dist/* /opt/wheels ; \
  rm -rf /opt/mybuild 
  
RUN $WITH_CONDA; set -eux ; \
  pip install /opt/wheels/apex-*.whl

RUN $WITH_CONDA; set -eux ; \
  ROCM_VERSION_MAJOR=$(echo $ROCM_RELEASE | cut -d'.' -f1) ; \
  ROCM_VERSION_MINOR=$(echo $ROCM_RELEASE | cut -d'.' -f2) ; \
  PYTHON_VERSION_MAJOR=$(echo $PYTHON_VERSION | cut -d'.' -f1) ; \
  PYTHON_VERSION_MINOR=$(echo $PYTHON_VERSION | cut -d'.' -f2) ; \
  pip3 install --pre torchvision==0.17.2+rocm${ROCM_VERSION_MAJOR}.${ROCM_VERSION_MINOR} --index-url https://download.pytorch.org/whl/ ; \
  pip3 install --pre torchdata==0.7.1 --index-url https://download.pytorch.org/whl/ ; \
  pip3 install --pre torchtext==0.17.2 --index-url https://download.pytorch.org/whl/ ; \
  pip3 install --pre torchaudio==${PYTORCH_VERSION}+rocm${ROCM_VERSION_MAJOR}.${ROCM_VERSION_MINOR} --index-url https://download.pytorch.org/whl/ ; \
  true

ENV RUSTUP_HOME /opt/rust
ENV CARGO_HOME /opt/rust
RUN set -eux ; \
  curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs > /opt/rust.sh ; \
  sh /opt/rust.sh -y --no-modify-path ; \
  rm -rf /opt/rust.sh

ENV PATH $PATH:/opt/rust/bin

RUN $WITH_CONDA; set -eux ; \
  ln -s $(which gcc-10) /opt/rust/bin/cc ; \
  CC=gcc-10 CXX=g++-10 \
  DS_BUILD_AIO=0 \
  DS_BUILD_CCL_COMM=1 \
  DS_BUILD_CPU_ADAM=0 \
  DS_BUILD_CPU_LION=0 \
  DS_BUILD_EVOFORMER_ATTN=0 \
  DS_BUILD_FUSED_ADAM=1 \
  DS_BUILD_FUSED_LION=1 \
  DS_BUILD_CPU_ADAGRAD=0 \
  DS_BUILD_FUSED_LAMB=1 \
  DS_BUILD_QUANTIZER=0 \
  DS_BUILD_RANDOM_LTD=0 \
  DS_BUILD_SPARSE_ATTN=0 \
  DS_BUILD_TRANSFORMER=0 \
  DS_BUILD_TRANSFORMER_INFERENCE=0 \
  DS_BUILD_STOCHASTIC_TRANSFORMER=1 \
  pip install deepspeed==0.14.0 --global-option="build_ext" --global-option="-j32" ; \
  ds_report ; \
  true

# Tested with flash-attention 2554f49
RUN $WITH_CONDA; set -eux ; \
  cd /opt ; \
  curl -LO http://localhost:$SERVER_PORT/rocm-5.6.0.tar ; \
  tar -xf rocm-5.6.0.tar ; \
  \
  cd $ROCM_PATH/bin ; \
  curl -LO http://localhost:$SERVER_PORT/hipcc-flash-attention ; \
  chmod +x hipcc-flash-attention ; \
  mv hipcc hipcc.bak ; \
  mv hipcc-flash-attention hipcc ; \
  \
  git clone --recursive https://github.com/ROCmSoftwarePlatform/flash-attention.git /opt/mybuild ; \
  cd /opt/mybuild ; \
  cp -rf benchmarks /opt/wheels/flash_attn-benchmarks ; \
  \
  rm -rf build ; \
  CC=gcc-10 \
  CXX=g++-10 \
  GPU_ARCHS="gfx90a" \
    python setup.py bdist_wheel ; \
  cp -rf dist/* /opt/wheels ; \
  \
  rm -rf /opt/rocm-5.6.0* ; \
  rm -rf $ROCM_PATH/bin/hipcc ; \
  mv $ROCM_PATH/bin/hipcc.bak $ROCM_PATH/bin/hipcc ; \
  rm -rf /opt/mybuild 

RUN $WITH_CONDA; set -eux ; \
  pip install /opt/wheels/flash_attn-*.whl

#Tested with xformers 82368ac
RUN $WITH_CONDA; set -eux ; \
  cd /opt ; \
  curl -LO http://localhost:$SERVER_PORT/rocm-5.7.1.tar ; \
  tar -xf rocm-5.7.1.tar ; \
  \
  cd $ROCM_PATH/bin ; \
  curl -LO http://localhost:$SERVER_PORT/hipcc-xformers ; \
  chmod +x hipcc-xformers ; \
  mv hipcc hipcc.bak ; \
  mv hipcc-xformers hipcc ; \
  \
  git clone --recursive -b main https://github.com/ROCm/xformers /opt/mybuild ; \
  cd /opt/mybuild ; \
  \
  rm -rf build ; \
  CC=gcc-10 \
  CXX=g++-10 \
  HIP_ARCHITECTURES="gfx90a" \
  PYTORCH_ROCM_ARCH="gfx90a" \
  COMPILE_ROCM_PATH=/opt/rocm-5.7.1 \
  MAX_JOBS=48 \
    python setup.py bdist_wheel ; \
  cp -rf dist/* /opt/wheels ; \
  \
  rm -rf /opt/rocm-5.7.1* ; \
  rm -rf $ROCM_PATH/bin/hipcc ; \
  mv $ROCM_PATH/bin/hipcc.bak $ROCM_PATH/bin/hipcc ; \
  rm -rf /opt/mybuild 

RUN $WITH_CONDA; set -eux ; \
  pip install /opt/wheels/xformers-*.whl

RUN $WITH_CONDA; set -eux ; \
  pip install \
    scipy==1.12.0 \
    matplotlib==3.8.2 \
    pandas==2.2.0 \
    seaborn==0.13.2

# Work around wierd interaction with python interpreter in Triton.
RUN set -eux ; \
  sed -i 's#assert(False.*#assert is_hip(), "unsupported target"#g' /opt/miniconda3/envs/pytorch/lib/python3.10/site-packages/triton/compiler/compiler.py
#  sed -i 's#mod, func, n_regs, n_spills#assert True; mod, func, n_regs, n_spills#g' /opt/miniconda3/envs/pytorch/lib/python3.10/site-packages/triton/compiler/compiler.py 

# 
# Bits and bytes
#
RUN $WITH_CONDA; set -eux ; \ 
  git clone https://github.com/ROCm/bitsandbytes /opt/mybuild ; \
  cd /opt/mybuild ; \
  git checkout -b mydev 43d3976 ; \
  pip install -r requirements-dev.txt ; \
  cmake -DCOMPUTE_BACKEND=hip -DBNB_ROCM_ARCH="gfx90a" -S . ; \
  nice make -j ; \
  python setup.py bdist_wheel ; \
  cp dist/bitsandbytes-*.whl /opt/wheels ; \
  true

RUN $WITH_CONDA; set -eux ; \ 
  pip install /opt/wheels/bitsandbytes-*.whl

#
# Some other packages we may need
#
RUN $WITH_CONDA; set -eux ; \ 
  pip install \
    transformers==4.41.2 \
    sentencepiece==0.2.0 \
    protobuf==5.27.1 \
    accelerate==0.31.0 

#
# Attempt to fix memory accounting error in ROCm 5.6.1
#
RUN $WITH_CONDA; set -eux ; \
  cd $ROCM_PATH/lib ; \
  curl -LO http://localhost:$SERVER_PORT/memgetinfo-fix.cpp ; \
  echo "9e5ac400275f318d9fe94c55d769b5990374eeb30120e4b8dc61634218238a34  memgetinfo-fix.cpp" | shasum -c - ; \
  mv memgetinfo-fix.cpp preload-me.cpp ; \
  bash preload-me.cpp ; \
  rm -rf preload-me.cpp

RUN set -eux ; \
  echo 'if [[ "$@" == "pytorch" ]] ; then export LD_PRELOAD=$ROCM_PATH/lib/libpreload-me.so ; fi' >> /opt/miniconda3/bin/activate
